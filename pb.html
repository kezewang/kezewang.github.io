
<!DOCTYPE html>
<!-- saved from url=(0051)http://getbootstrap.com/examples/navbar-fixed-top/# -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->

    <meta name="description" content="Keze&#39;s personal website">
    <meta name="author" content="wangkeze">
    <link rel="icon" href="">
    <title>Keze Wang (王可泽)</title>

    <!-- Bootstrap core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/zoom.css" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <link href="css/ie10-viewport-bug-workaround.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/navbar-fixed-top.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <style type="text/css">
        body {
            font-family:georgia, serif;
            font-size: 15px;
            background-color: #edf0f5;
        }

        .jumbotron {
            background-color: #fff;
        }

        .jumbotron p {
            font-size: inherit;
        }

        .kpanel {
            background-color: none;
            border: none;
            box-shadow: none;
            margin-bottom: 25px;
        }

        .kpanel > .panel-heading {
            color: inherit;
            font-weight: 600;
            padding: 10px 4px;
            transition: all .3s;
            border: 1px solid transparent;
            background: #fff;
            border-color: #e4e5e7;
            border: 1px solid #e4e5e7;
            padding: 10px 10px;
            border-radius: 2px;
        }

        .kpanel > .panel-body {
            background: #fff;
            border: 1px solid #e4e5e7;
            border-radius: 2px;
            padding: 20px;
            position: relative;
            border-top: 2px solid #3498db;
        }

        .dd {
            position: relative;
            display: block;
            margin: 0;
            padding: 0;
            list-style: none;
            font-size: 13px;
            line-height: 20px;
        }

        .dd-list {
            display: block;
            position: relative;
            margin: 0;
            padding: 0;
            list-style: none;
        }

        .dd-list .dd-list {
            padding-left: 30px;
            text-align: justify;
            color: #333;
        }

        .dd-collapsed .dd-list {
            display: none;
            color: black;
            text-align: justify;
            color: #333;
        }

        .dd-item,
        .dd-empty,
        .dd-placeholder {
            color: black;
            display: block;
            position: relative;
            margin: 0;
            padding: 0;
            min-height: 20px;
            font-size: 13px;
            line-height: 20px;
            text-align: justify;
            color: #333;
        }

        .dd-handle {
            display: block;
            margin: 5px 0;
            padding: 5px 10px;
            text-decoration: none;
            border: 1px solid #e4e5e7;
            background: #f7f9fa;
            -webkit-border-radius: 3px;
            border-radius: 3px;
            box-sizing: border-box;
            -moz-box-sizing: border-box;
        }

        .dd-handle span {
            font-weight: bold;
        }

        .dd-handle:hover {
            background: #f0f0f0;
            cursor: pointer;
            font-weight: bold;
        }

        .dd-item > button {
            display: block;
            position: relative;
            cursor: pointer;
            float: left;
            width: 25px;
            height: 20px;
            margin: 5px 0;
            padding: 0;
            text-indent: 100%;
            white-space: nowrap;
            overflow: hidden;
            border: 0;
            background: transparent;
            font-size: 12px;
            line-height: 1;
            text-align: center;
            font-weight: bold;
        }

        .dd-item > button:before {
            content: '+';
            display: block;
            position: absolute;
            width: 100%;
            text-align: center;
            text-indent: 0;
        }

        .dd-item > button[data-action="collapse"]:before {
            content: '-';
        }

        .dd-handle:hover {
            font-weight: inherit;
        }

        .card-block {
            padding: 1.25rem;
            border-top: 1px solid #777;
        }

        .label-value {
            padding-top: 7px;
        }

        .kgreen > .panel-body {
            border: none;
            border-bottom: 2px solid #62cb31;
            /*border-top: 2px solid #62cb31;*/
        }

        .project-img {
            margin-top: 20px;
            max-width: 100%;
            height: auto;
        }

        .my-nav {
            font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
        }

        .my-link:hover {
            color: #3498db!important;
        }

        .my-link:focus {
            color: #3498db!important;
        }

        .my-link:active {
            color: #3498db!important;
        }

        a.active {
            color: #3498db!important;
            background-color: inherit!important; 
        }

        .navbar-default .navbar-nav>.active>a, .navbar-default .navbar-nav>.active>a:focus, .navbar-default .navbar-nav>.active>a:hover {
            color: #3498db!important;
            background-color: inherit!important; 
        }

        ul.my-ul li {
            margin-bottom: 15px;
        }

        h3 {
            color: #3498db!important;
            font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
        }

        li.my-ul > a {
            color: #3498db!important;
        }
	.center-block {
            text-align: center;
	    margin: 10px 0;
        }
    </style>
</head>

<body>

    <!-- Fixed navbar -->
    <nav class="navbar navbar-default navbar-fixed-top my-nav">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand my-link" href="./index.html">Keze Wang (王可泽)</a>
            </div>
            <div id="navbar" class="navbar-collapse collapse">
                <ul class="nav navbar-nav">
                    <li>
                        <a class="my-link active" href="#">Publications</a>
                    </li>
                    <li>
                        <a class="my-link" href="./proj.html">Selected Projects</a>
                    </li>
                    <li>
                        <a class="my-link" href="./tutorials.html">Tutorials</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container">
        <div class="jumbotron">
            <h3>Journal Papers</h3>
            <ul class="my-ul">
		<li>
                    <b>Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Diagnosis. To appear in <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (T-PAMI), 2023.</b>
                    [<a href="https://arxiv.org/abs/2003.06534">paper</a>]
                    <br/>
			<span>Junfan Lin, <b>Keze Wang*</b>, Ziliang Chen, Xiaodan Liang, Liang Lin</span>
                </li>

		<li>
                    <b>Multi-Person 3D Pose Estimation With Occlusion Reasoning. To appear in <em>IEEE Transactions on Multimedia</em> (T-MM), 2023.</b>
                    [<a href="https://ieeexplore.ieee.org/abstract/document/10117604">paper</a>]
                    <br/>
			<span>Xipeng Chen, Junzheng Zhang, <b>Keze Wang</b>, Pengxu Wei, Liang Lin*</span>
                </li>
		    
		<li>
                    <b>TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning. In <em>IEEE Transactions on Image Processing</em> (T-IP), 2022.</b>
                    [<a href="https://arxiv.org/pdf/2112.03587.pdf">paper</a>]
                    <br/>
		            <span>Yang Liu, <b>Keze Wang*</b>, Lingbo Liu, Haoyuan Lan, Liang Lin</span> 
                </li>
		        <li>
		            <b>CX-ToM: Counterfactual explanations with theory-of-mind for enhancing human trust in image recognition models. In iScience Cell Press Journal 2022.</b>
                    [<a href="https://www.cell.com/iscience/pdf/S2589-0042(21)01551-0.pdf">paper</a>]
		            <br/>
		            <span>Arjun R Akula*, <b>Keze Wang*</b>, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai, Song-Chun Zhu</span>
		        </li>
		        <li>
                    <b>Semantics-aware adaptive knowledge distillation for sensor-to-vision action recognition. In <em>IEEE Transactions on Image Processing</em> (T-IP), 2021.</b>
                    [<a href="https://arxiv.org/pdf/2009.00210.pdf">paper</a>]
                    <br/>
		            <span>Yang Liu, <b>Keze Wang*</b>, Guanbin Li, Liang Lin</span> 
                </li>
                <li>
                    <b>Knowledge-Routed Visual Question Reasoning: Challenges for Deep Representation Embedding. In <em>IEEE Transactions on Neural Networks and Learning Systems</em> (T-NNLS), 2021.</b>
                    [<a href="https://arxiv.org/abs/2012.07192">paper</a>][<a href="https://github.com/qingxingcao/KRVQA">Dataset</a>][<a href="https://mp.weixin.qq.com/s/EkOEgU9_S-ozYUs2BmL5Cg">Chinese Media Report</a>]
                    <br/>
                    <span>Qingxing Cao, Bailin Li, Xiaodan Liang, <b>Keze Wang</b>, Liang Lin*</span> 
                </li>
                <li>
                    <b>3D Human Pose Machines with Self-supervised Learning. in <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (T-PAMI), vol. 42, no. 5, pp. 1069-1082, 2020.</b>
                    [<a href="https://arxiv.org/abs/1901.03798">paper</a>][<a href="https://github.com/chanyn/3Dpose_ssl">code</a>][<a href="http://www.sysu-hcp.net/3d_pose_ssl/">ProjectPage</a>][<a href="https://www.zdnet.com/article/chinas-ai-scientists-teach-a-neural-net-to-train-itself/">Media Report</a>]
                    <br/>
                    <span><b>Keze Wang</b>, Liang Lin*, Chenhan Jiang, Chen Qian, and Pengxu Wei.</span>
                </li>
                <li>
                    <b>Cost-Effective Object Detection: Active Sample Mining with Switchable Selection Criteria. In <em>IEEE Transactions on Neural Networks and Learning Systems</em> (T-NNLS), vol. 30, no. 3, pp. 834-850, 2019.</b>
                    [<a href="../doc/ASM_preprint.pdf">paper</a>][<a href="http://www.sysu-hcp.net/asm/">ProjectPage</a>][<a href="../codes/ASM_ver1.rar">code</a>][<a href="https://github.com/yanxp/ASM-Pytorch">code in pytorch</a>]     <br/>
                    <span><b>Keze Wang</b>, Liang Lin*, Xiaopeng Yan, Ziliang Chen, and Lei Zhang. </span> 
                </li>
                <li>
                    <b>Active Self-Paced Learning for Cost-Effective and Progressive Face Identification. In <em>IEEE Transactions on Pattern Analysis and Machine Intelligence </em> (T-PAMI), vol. 40, no. 1, pp. 7-19, 2018.</b>
                    [<a href="https://arxiv.org/abs/1701.03555">paper</a>][<a href="https://github.com/kezewang/ASPL">code</a>][<a href="http://www.sysu-hcp.net/active-self-paced-learning-for-cost-effective-and-progressive-face-identification/">ProjectPage</a>]
                    <br/>
                    <span>Liang Lin*, <b>Keze Wang</b>, Deyu Meng, Wangmeng Zuo, and Lei Zhang. </span> 
                </li>
                <li>
                    <b>
                        Cost-Effective Active Learning for Deep Image Classification. In <em>IEEE Transactions on Circuits and Systems for Video Technology</em> (T-CSVT), vol. 27, no. 12, pp. 2591-2600, 2017.
                    </b>
                    [<a href="https://arxiv.org/abs/1701.03551">paper</a>]
                    <br/>
                    <span><b>Keze Wang</b>, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. </span> 
                </li>
                <li>
                    <b>
                        Structure-Preserving Image Super-resolution via Contextualized Multi-task Learning. In <em>IEEE Transactions on Mulitmedia</em> (TMM), vol. 19, no. 12, pp. 2804-2815, 2017.
                    </b>
                    [<a href="https://arxiv.org/abs/1707.08340">paper</a>][<a href="http://www.sysu-hcp.net/structure-preserving-image-super-resolution-via-contextualized-multi-task-learning/">ProjectPage</a>][<a href="https://github.com/ykshi/SPNet">Code</a>]
                    <br />
                    <span>Yukai Shi, <b>Keze Wang</b>, Chongyu Chen*, Li Xu and Liang Lin. </span>
                </li>
                <li>
                    <b>
                        Deep Co-Space: Sample Mining Across Feature Transformation for Semi-Supervised Learning. In <em>IEEE Transactions on Circuits and Systems for Video Technology</em> (T-CSVT), vol. 28, no. 10, pp. 2667-2678, 2017.
                    </b>
                    [<a href="https://arxiv.org/abs/1707.09119">paper</a>][<a href="http://www.sysu-hcp.net/dcs/">ProjectPage</a>]
                    <br />
                    <span>Ziliang Chen, <b>Keze Wang</b>, Xiao Wang, Pai Peng and Liang Lin*. </span>
                </li>
                <li>
                    <b>A Deep Structured Model with Radius–Margin Bound for 3D Human Activity Recognition. In <em>International Journal of Computer Vision</em> (IJCV), vol. 118, no. 2, pp. 256-273, 2016.</b>
                    [<a href="doc/IJCV_3DHumanActivity.pdf">paper</a>]
                    <br />
                    <span>Liang Lin*, <b>Keze Wang</b>, Wangmeng Zuo, Meng Wang, Jiebo Luo, and Lei Zhang. </span>
                </li>
                <li>
                    <b>PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Edge-Preserving Coherence. In <em>IEEE Transactions on Image Processing</em> (T-IP), vol. 24, no. 10, pp. 3019-3033, 2015.</b>
                    [<a href="doc/PISA_published.pdf">paper</a>][<a href="https://github.com/kezewang/pixelwiseImageSaliencyAggregation">code</a>][<a href="http://www.sysu-hcp.net/pixelwise-image-saliency-by-aggregation/">ProjectPage</a>]
                    <br />
                    <span><b>Keze Wang</b>, Liang Lin*, Jiangbo Lu, Chenglong Li, and Keyang Shi. </span>
                </li>
            </ul>

            <h3>Conference Papers</h3>
        <h4>2025</h4>
		 <ul class="my-ul">
		
		<li>
			    <b>AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay. In <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining </em> (KDD), 2025. </b> 
                    <br/>
		          <span>Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, <b>Keze Wang*</b>, Jinrui Chen, Liang Lin.</span>
		</li>	
		<li>
			    <b>KABB: Knowledge-Aware Bayesian Bandits for Dynamic Expert Coordination in Multi-Agent Systems. In <em>International Conference on Machine Learning</em> (ICML), 2025. </b> 
                    <br/>
		          <span>Jusheng Zhang, Zimeng Huang, Yijia Fan, Ningyuan Liu, Mingyan Li, Zhuojie Yang, Jiawei Yao, Jian Wang, <b>Keze Wang*</b>.</span>
		</li>	
		<li>
			    <b> Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2025. </b> 
                    <br/>
		          <span>Zeqing Wang, Qingyang Ma, Wentao Wan, Haojie Li, <b>Keze Wang*</b>, Yonghong Tian.</span>
		</li>	
			 
		<li>
			    <b>SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language Models Tackling Knowledge-based Reasoning Tasks. In <em>Annual AAAI Conference on Artificial Intelligence</em> (AAAI), 2025. </b> 
                    <br/>
		          <span>Wentao Wan, Zhuojie Yang, Yongcan Chen, Chenglin Luo, Ruilin Wang, Kehao Cai, Nan Kang, Liang Lin, <b>Keze Wang*</b>.</span>
		</li>			 
		 </ul>
			
	    <h4>2024</h4>
	    <ul class="my-ul">
		<li>
                  <b>Adaptive Prompt Routing for Arbitrary Text Style Transfer with Pre-trained Language Models. In <em>Annual AAAI Conference on Artificial Intelligence</em> (AAAI), 2024. </b> 
                    <br/>
		          <span>Qingyi Liu, Jinhui Qin, Wenxuan Ye, Hao Mou, Yuxuan He, <b>Keze Wang*</b>.</span>
                </li>
		<li>
		  <b>NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields via View Prompt Tuning. In <em>Annual AAAI Conference on Artificial Intelligence</em> (AAAI), 2024. </b> 
                    <br/>
		          <span>Linsheng Chen, Guangrun Wang, Liuchun Yuan, <b>Keze Wang*</b>, Ken Deng, Philip H.S. Torr.</span>
		</li>
		<li>
		  <b>Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation. To appear in <em>Annual AAAI Conference on Artificial Intelligence</em> (AAAI), 2024. </b> 
                    <br/>
		          <span>Hui Fu, Zeqing Wang, Ke Gong, <b>Keze Wang</b>, Tianshui Chen, Haojie Li, Haifeng Zeng, Wenxiong Kang*.</span>
		</li>
		<li>
		  <b>Gesture Generation via Diffusion Model with Attention Mechanism. To appear in <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em> (ICASSP), 2024. </b> 
                    <br/>
		          <span>Lingling Li, Weicong Li, Qiyuan Ding, Chengpei Tang, <b>Keze Wang*</b>.</span>
		</li>
	    </ul>
            <h4>2021</h4>
            <ul class="my-ul">
                <li>
                  <b>Mind the Context: The Impact of Contextualization in Neural Module Networks for Grounding Visual Referring Expressions. In <em>Proc. of Conference on Empirical Methods in Natural Language Processing</em> (EMNLP), 2021. </b> 
                  [<a href="https://aclanthology.org/2021.emnlp-main.516/">paper</a>]
                    <br/>
		          <span>Arjun Akula, Spandana Gella, <b>Keze Wang</b>, Song-Chun Zhu, Siva Reddy.</span>
                </li>
            </ul>
            <ul class="my-ul">
                <li>
                  <b>Solving inefficiency of self-supervised representation learning. In <em>Proc. of International Conference on Conference on Computer Vision</em> (ICCV), 2021. </b> 
                  [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Solving_Inefficiency_of_Self-Supervised_Representation_Learning_ICCV_2021_paper.pdf">paper</a>]
                    <br/>
		          <span>Guangrun Wang, <b>Keze Wang</b>, Guangcong Wang, Philip HS Torr, Liang Lin.</span>
                </li>
            </ul>
            <ul class="my-ul">
                <li>
                  <b>Linguistically Routing Capsule Network for Out-of-Distribution Visual Question Answering. In <em>Proc. of International Conference on Conference on Computer Vision</em> (ICCV), 2021. </b> 
                  [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Cao_Linguistically_Routing_Capsule_Network_for_Out-of-Distribution_Visual_Question_Answering_ICCV_2021_paper.pdf">paper</a>]
                    <br/>
		          <span>Qingxing Cao, Wentao Wan, <b>Keze Wang</b>, Xiaodan Liang, Liang Lin*</span>
                </li>
            </ul>
            <ul class="my-ul">
                <li>
                  <b>Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp. In <em>Proc. of International Conference on Robotics and Automation</em> (ICRA), 2021. </b> [<a href="https://arxiv.org/abs/2011.14487">paper</a>][<a href="https://github.com/junfanlin/continuous-transition">code</a>][<a href="./videos/ICRA21.mp4">video</a>]
                    <br/>
                    <span>Junfan Lin, Zhongzhan Huang, <b>Keze Wang*</b>, Xiaodan Liang, Weiwei Chen, Liang Lin.</span>
                </li>
            </ul>
            <h4>2020</h4>
            <ul class="my-ul">
                <li>
                  <b>Grammatically Recognizing Images with Tree Convolution. In <em>Proc. of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining</em> (KDD), 2020. </b> [<a href="https://dl.acm.org/doi/pdf/10.1145/3394486.3403133">paper</a>][<a href="https://github.com/wanggrun/TreeConv">code</a>]
                    <br/>
                    <span>Guangrun Wang, Guangcong Wang, <b>Keze Wang</b>, Xiaodan Liang, and Liang Lin*.</span>
                </li>
            </ul>
            <h4>2019</h4>
            <ul class="my-ul">
                <li>
                  <b>Adaptively Connected Neural Networks. In <em>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2019. </b> [<a href="https://arxiv.org/abs/1904.03579">paper</a>][<a href="https://github.com/wanggrun/Adaptively-Connected-Neural-Networks">code</a>]
                    <br/>
                    <span>Guangrun Wang, <strong>Keze Wang</strong>, Liang Lin*. </span>
                </li>
            </ul>
            <h4>2018</h4>
            <ul class="my-ul">
                <li>
                  <b>Convolutional Memory Blocks for Depth Data Representation Learning. In <em>Proc. of International Joint Conference on Artificial Intelligence</em> (IJCAI), 2018.</b> [<a href="../doc/IJCAI2018_CMB.pdf">paper</a>]
                    <br/>
                    <span><strong>Keze Wang</strong>, Liang Lin*, Chuangjie Ren, Wei Zhang, Wenxiu Sun. </span>
                </li>
                <li>
                    <b>Towards Human-Machine Cooperation: Self-supervised Sample Mining for Object Detection. In <em>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2018. </b>
                    [<a href="../doc/CVPR2018_SSM.pdf">paper</a>][<a href="http://www.sysu-hcp.net/ssm/">ProjectPage</a>][<a href="../codes/SSM_CVPR.zip">code</a>][<a href="https://github.com/yanxp/SSM-Pytorch">code in pytorch</a>][<a href="https://blog.csdn.net/qq_24548569/article/details/83687881?utm_medium=distribute.pc_relevant.none-task-blog-searchFromBaidu-13.control&dist_request_id=1328680.52651.16164183412711033&depth_1-utm_source=distribute.pc_relevant.none-task-blog-searchFromBaidu-13.control">Chinese Briefing</a>]     
                    <br/>
                    <span><strong>Keze Wang</strong>, Xiaopeng Yan, Donyu Zhang, Lei Zhang, Liang Lin*. </span>
                </li>
                <!--li>
                    <b>Flow Guided Recurrent Neural Encoder for Video Salient Object Detection. In <em>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2018. </b>
                    [<a href="../doc/CVPR2018_FRGNE.pdf">paper</a>]     
                    <br/>
                    <span>Guanbin Li, Yuan Xie, Tianhao Wei, <strong>Keze Wang</strong>, Liang Lin*. </span>
                </li-->
            </ul>
            <h4>2017</h4>
            <ul class="my-ul">
                <li>
                    <b>Recurrent 3D Pose Sequence Machines. In <em>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2017. </b>
                    [<a href="doc/CVPR17_R3DPose.pdf">paper</a>][<a href="https://github.com/Geekking/RPSM">code</a>]
                    <br />
                    <span>Mude Lin, Liang Lin, Xiaodan Liang, <strong>Keze Wang</strong>, and Hui Cheng*. </span>
                </li>
            </ul>
            <h4>2016</h4>
            <ul class="my-ul">
                <li>
                    <b>
                        Human Pose Estimation from Depth Images via Inference Embedded Multi-task Learning. In <em>Proceedings of the ACM International Conference on Multimedia</em> (ACM MM), 2016. (full paper, oral)
                    </b>
                    [<a href="doc/acm_mm_2016.pdf">paper</a>][<a href="http://www.sysu-hcp.net/kinect2-human-pose-dataset-k2hpd/">Dataset</a>]
                    <br />
                    <span><strong>Keze Wang</strong>, Shengfu Zhai, Hui Cheng, Xiaodan Liang, and Liang Lin*. </span>
                </li>
                <li>
                    <b>Dictionary Pair Classifier Driven Convolutional Neural Networks for Object Detection. In <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2016. </b>
                    [<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Wang_Dictionary_Pair_Classifier_CVPR_2016_paper.pdf">Paper</a>]
                    <br />
                    <span><strong>Keze Wang</strong>, Liang Lin*, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. </span>
                </li>
                <li>
                    <b> Local- and Holistic- Structure Preserving Image Super-Resolution via Deep Joint Component Learning. In <em>Proceedings of the IEEE International Conference on Multimedia and Expo</em> (ICME), 2016. (oral) </b>
                    [<a href="doc/ICME2016_SR.pdf">paper</a>]
                    <br />
                    <span>Yukai Shi, <strong>Keze Wang</strong>, Li Xu, Liang Lin*.</span>
                </li>
                <li>
                    <b>Learning a Lightweight Deep Convolutional Network for Joint Age and Gender Recognition. In <em>Proceedings of the IEEE International Conference on Pattern Recognition</em> (ICPR), 2016. (oral)</b>
                    [<a href="doc/ICPR2016.pdf">paper</a>]
                    <br />
                    <span>Linnan Zhu, <strong>Keze Wang</strong>, Liang Lin, Lei Zhang*. </span>
                </li>
            </ul>
            <h4>2014</h4>
            <ul class="my-ul">
                <li>
                    <b>3D Human Activity Recognition with Reconfigurable Convolutional Neural Networks.
                    In <em>ACM International Conference on Multimedia (ACM MM), 2014</em>. (full paper, oral)</b>
                    [<a href="doc/ACMMM14_StrucCNN_Final.pdf">paper</a>][<a href="http://www.sysu-hcp.net/resources/">dataset description</a>][<a href="https://pan.baidu.com/s/1mi0XgV2#list/path=%2F">dataset download</a>]
                    <br />
                    <span><strong>Keze Wang</strong>, Xiaolong Wang, and Liang Lin*. </span>
                </li>
            </ul>
            <h4>2013</h4>
            <ul class="my-ul">
                <li>
                    <b>PISA: Pixelwise Image Saliency by Aggregating Complementary Appearance Contrast Measures with Spatial Priors. In <em>Proc. of IEEE Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2013. </b> [<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Shi_PISA_Pixelwise_Image_2013_CVPR_paper.pdf">paper</a>]
                    <br />
                    <span>Keyang Shi, <strong>Keze Wang</strong>, Jiangbo Lu, and Liang Lin*. </span>
                </li>
            </ul>      

        </div>

    </div>
    <div class="center-block">
        <a href="http://www.beian.miit.gov.cn">粤ICP备17051736号-1</a>
    </div>
    <!-- /container -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/jquery.nestable.js"></script>
    <script src="js/zoom.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <script src="js/ie10-viewport-bug-workaround.js"></script>
    <script type="text/javascript">
        $(".nav a").on("click", function(){
           $(".nav").find(".active").removeClass("active");
           $(this).parent().addClass("active");
        });

        $('.dd').nestable({});

    </script>

</body>
</html>
